{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated value of the policy (U(π)): 0.24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the Grid World Environment\n",
    "class GridWorld:\n",
    "    def __init__(self, grid_size, start_state, end_state, rewards, gamma=0.9):\n",
    "        self.grid_size = grid_size\n",
    "        self.start_state = start_state\n",
    "        self.end_state = end_state\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        # Possible actions: 'up', 'down', 'left', 'right'\n",
    "        x, y = state\n",
    "        if action == 'up':\n",
    "            next_state = (max(x - 1, 0), y)\n",
    "        elif action == 'down':\n",
    "            next_state = (min(x + 1, self.grid_size - 1), y)\n",
    "        elif action == 'left':\n",
    "            next_state = (x, max(y - 1, 0))\n",
    "        elif action == 'right':\n",
    "            next_state = (x, min(y + 1, self.grid_size - 1))\n",
    "        else:\n",
    "            next_state = state  \n",
    "        \n",
    "        reward = self.rewards.get(next_state, 0)  \n",
    "        done = next_state == self.end_state\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        return self.start_state\n",
    "\n",
    "# Define a random policy\n",
    "def random_policy(state):\n",
    "    return random.choice(['up', 'down', 'left', 'right'])\n",
    "\n",
    "# Monte Carlo Policy Evaluation\n",
    "def monte_carlo_policy_evaluation(env, policy, num_episodes=1000, max_steps=50):\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        trajectory = []\n",
    "        total_reward = 0\n",
    "        discount = 1\n",
    "        \n",
    "        # Generate a trajectory\n",
    "        for _ in range(max_steps):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done = env.step(state, action)\n",
    "            trajectory.append((state, action, reward))\n",
    "            total_reward += discount * reward\n",
    "            discount *= env.gamma\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "    \n",
    "    # Estimate U(π) as the average return across all episodes\n",
    "    U_pi = np.mean(total_rewards)\n",
    "    return U_pi\n",
    "\n",
    "\n",
    "grid_size = 5\n",
    "start_state = (0, 0)\n",
    "end_state = (4, 4)\n",
    "rewards = {(4, 4): 10}  \n",
    "gamma = 0.9\n",
    "\n",
    "env = GridWorld(grid_size, start_state, end_state, rewards, gamma)\n",
    "\n",
    "# Perform Monte Carlo Policy Evaluation\n",
    "U_pi = monte_carlo_policy_evaluation(env, random_policy, num_episodes=1000, max_steps=50)\n",
    "\n",
    "print(f\"Estimated value of the policy (U(π)): {U_pi:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giải thích:\n",
    "1.Môi trường (GridWorld):\n",
    "\n",
    " Một grid 5×5 với trạng thái bắt đầu ở góc trên trái  (0,0) và mục tiêu ở góc dưới phải (4,4) \n",
    " Phần thưởng là 10 khi đến mục tiêu (4,4) các trạng thái khác không có phần thưởng.\n",
    " Agent di chuyển theo chính sách ngẫu nhiên (random policy).\n",
    "\n",
    "2.Monte Carlo Policy Evaluation:\n",
    "\n",
    " Mỗi trajectory bắt đầu từ trạng thái khởi đầu (0,0) và kết thúc khi đạt trạng thái mục tiêu hoặc vượt quá số bước tối đa.\n",
    " Giá trị U(π) được tính là trung bình tổng phần thưởng R(τ) trên tất cả các trajectory.\n",
    "\n",
    "Kết quả:\n",
    "U(π): Giá trị kỳ vọng của chính sách ngẫu nhiên (random policy), được ước lượng bằng cách lấy trung bình từ 1000\n",
    "episode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
